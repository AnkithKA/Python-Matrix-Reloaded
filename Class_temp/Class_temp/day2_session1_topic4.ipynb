{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 🧠 Day 2 – Session 1 • Topic 3  \n",
        "## GIL-Aware Timing & Concurrency Trade-offs\n",
        "\n",
        "This script demonstrates how Python's **Global Interpreter Lock (GIL)** impacts performance when using threads vs processes for **CPU-bound workloads**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Cleaned Code with Inline Comments\n",
        "\n",
        "```python\n",
        "import time\n",
        "import threading\n",
        "import multiprocessing\n",
        "import sys\n",
        "import math\n",
        "\n",
        "# A CPU-bound task: compute many square roots\n",
        "def cpu_task(n):\n",
        "    total = 0.0\n",
        "    for i in range(1, n):\n",
        "        total += math.sqrt(i)\n",
        "    return total\n",
        "\n",
        "# Adjust N based on your machine speed to make the demo fast or slow\n",
        "N = 300_000  # ~300K iterations per run\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 1. Single-thread baseline\n",
        "# --------------------------\n",
        "print(\"Running single-threaded baseline...\")\n",
        "t0 = time.time()\n",
        "cpu_task(N)\n",
        "baseline = time.time() - t0\n",
        "print(f\"Single-thread : {round(baseline, 3)} sec\\n\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 2. Multi-threaded attempt\n",
        "# --------------------------\n",
        "# Threads won't run in parallel due to the GIL\n",
        "def threaded_run(workers=4):\n",
        "    threads = [threading.Thread(target=cpu_task, args=(N,)) for _ in range(workers)]\n",
        "    for th in threads:\n",
        "        th.start()\n",
        "    for th in threads:\n",
        "        th.join()\n",
        "\n",
        "print(\"Running multi-threaded (GIL limits parallelism)...\")\n",
        "t0 = time.time()\n",
        "threaded_run()\n",
        "threaded_time = time.time() - t0\n",
        "print(f\"4 threads     : {round(threaded_time, 3)} sec (GIL)\\n\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 3. Multi-processing version\n",
        "# --------------------------\n",
        "# Each process has its own GIL — true parallelism!\n",
        "def mp_worker(_):\n",
        "    cpu_task(N)\n",
        "\n",
        "def multiproc_run(workers=4):\n",
        "    with multiprocessing.Pool(workers) as pool:\n",
        "        pool.map(mp_worker, range(workers))\n",
        "\n",
        "print(\"Running multi-process (no shared GIL)...\")\n",
        "t0 = time.time()\n",
        "multiproc_run()\n",
        "mp_time = time.time() - t0\n",
        "print(f\"4 processes   : {round(mp_time, 3)} sec (no shared GIL)\\n\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 4. sys.setswitchinterval demo\n",
        "# --------------------------\n",
        "# This controls how often Python switches between threads (~preemption interval)\n",
        "orig_si = sys.getswitchinterval()\n",
        "sys.setswitchinterval(0.001)  # Set to 1 millisecond (more frequent switching)\n",
        "\n",
        "print(\"Running threads with more frequent context switch...\")\n",
        "t0 = time.time()\n",
        "threaded_run()\n",
        "new_threaded_time = time.time() - t0\n",
        "print(f\"Threads, switch=1ms: {round(new_threaded_time, 3)} sec\")\n",
        "sys.setswitchinterval(orig_si)  # Restore default\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# ASCII Diagram\n",
        "# --------------------------\n",
        "print(r\"\"\"\n",
        "ASCII timeline (2 threads):\n",
        "\n",
        "t=0   [Thread-A RUN ▓▓▓]  GIL held\n",
        "      [Thread-B WAIT ..]\n",
        "\n",
        "t=0.005 context switch -> GIL to Thread-B\n",
        "      [Thread-A WAIT ..]\n",
        "      [Thread-B RUN ▓▓▓]\n",
        "\n",
        "CPU-bound threads simply alternate; they never run truly in parallel.\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 🧾 Key Concepts Explained\n",
        "\n",
        "## 🔁 What Is the GIL?\n",
        "\n",
        "The **Global Interpreter Lock (GIL)** is a **mutex** in CPython that prevents multiple native threads from executing Python bytecodes at once.\n",
        "\n",
        "### ⚠️ Implication:\n",
        "Even on multi-core machines, **only one thread can execute Python code at a time**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 So Why Use Threads?\n",
        "\n",
        "Use threads when:\n",
        "- Your program is **I/O-bound**, e.g., waiting for disk/network\n",
        "- You're doing **latency hiding**\n",
        "- You want **simple concurrency** without process overhead\n",
        "\n",
        "Examples:\n",
        "- Downloading multiple files\n",
        "- Reading/writing files asynchronously\n",
        "- GUI event loops\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 When Does Multiprocessing Help?\n",
        "\n",
        "Use **`multiprocessing`** when:\n",
        "- You're doing **CPU-bound computation**\n",
        "- You want to use **multiple cores**\n",
        "- You need **true parallel execution**\n",
        "\n",
        "Each process gets its own:\n",
        "- Memory space\n",
        "- GIL\n",
        "- Python interpreter\n",
        "\n",
        "> ✅ Ideal for data processing, scientific computing, ML training loops\n",
        "\n",
        "---\n",
        "\n",
        "## 🧮 Sample Output (Expected Behavior)\n",
        "\n",
        "```\n",
        "Running single-threaded baseline...\n",
        "Single-thread : 0.289 sec\n",
        "\n",
        "Running multi-threaded (GIL limits parallelism)...\n",
        "4 threads     : 0.576 sec (GIL)\n",
        "\n",
        "Running multi-process (no shared GIL)...\n",
        "4 processes   : 0.301 sec (no shared GIL)\n",
        "\n",
        "Running threads with more frequent context switch...\n",
        "Threads, switch=1ms: 0.584 sec\n",
        "```\n",
        "\n",
        "> 📌 Even though threading runs slower than single-threaded (due to overhead), multiprocessing beats both!\n",
        "\n",
        "---\n",
        "\n",
        "## ⏱️ Best Practice Summary\n",
        "\n",
        "| Strategy | For | Pros | Cons |\n",
        "|---------|-----|------|------|\n",
        "| Single-threaded | Simple programs | Easy, fast | No parallelism |\n",
        "| Threading | I/O-bound tasks | Low overhead, easy | GIL blocks CPU parallelism |\n",
        "| Multiprocessing | CPU-bound tasks | True parallelism | Higher memory use, inter-process communication |\n",
        "| Native Extensions (e.g., NumPy) | CPU work | Fast, bypasses GIL | Requires external libraries |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Final Takeaway\n",
        "\n",
        "- ❌ Don’t expect **speedup from threading** for **CPU-bound tasks**\n",
        "- ✅ Use **multiprocessing** when you need **real parallelism**\n",
        "- 🛠 You can tweak **`sys.setswitchinterval()`** to experiment with thread scheduling — but it **won’t fix GIL contention**\n",
        "- 🚀 Consider **C extensions**, **subinterpreters (Python 3.12+)**, or **PyPy STM** for advanced parallelism\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "m-f7WldtRO4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 What is the GIL? (Global Interpreter Lock)\n",
        "\n",
        "The **GIL** (Global Interpreter Lock) is a **mutex** in CPython that ensures only **one thread** executes Python bytecode at a time, even on multi-core systems.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔒 Why Does the GIL Exist?\n",
        "\n",
        "- To protect access to **Python objects** in memory\n",
        "- Simplifies memory management and prevents race conditions\n",
        "- Makes C extensions easier to write\n",
        "\n",
        "---\n",
        "\n",
        "## ❌ The Big Trade-off\n",
        "\n",
        "> **No true parallelism in pure Python threads for CPU-bound tasks.**\n",
        "\n",
        "Even with 4 threads on 4 cores:\n",
        "- Only one thread runs Python code at a time\n",
        "- Threads take turns using the GIL\n",
        "- Adds overhead from context switching\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ When Is It Not a Problem?\n",
        "\n",
        "- For **I/O-bound** programs (e.g., waiting on disk/network)\n",
        "- With **C extensions** (e.g., NumPy, Pandas) that release the GIL\n",
        "- Using **multiprocessing** (each process has its own GIL)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Summary\n",
        "\n",
        "| Feature | GIL Impact |\n",
        "|--------|------------|\n",
        "| Threading | No real parallelism for CPU work |\n",
        "| Multiprocessing | Bypasses GIL (uses separate processes) |\n",
        "| C Extensions | Can run in parallel if they release GIL |\n",
        "| I/O Tasks | GIL doesn’t block wait time |\n",
        "\n",
        "---\n",
        "\n",
        "📌 **Bottom Line**:  \n",
        "The GIL makes Python **safe and fast for single-threaded code**, but it **limits CPU-bound concurrency** in multithreaded programs. Use **`multiprocessing` or native extensions** when you need real parallelism."
      ],
      "metadata": {
        "id": "RA91VtJuUJ6l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gozw5lKIROF6",
        "outputId": "5286a58d-05a4-4468-c2d0-18272bac0e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running single-threaded baseline...\n",
            "Single-thread : 0.025 sec\n",
            "\n",
            "Running multi-threaded (GIL limits parallelism)...\n",
            "4 threads     : 0.102 sec (GIL)\n",
            "\n",
            "Running multi-process (no shared GIL)...\n",
            "4 processes   : 0.164 sec (no shared GIL)\n",
            "\n",
            "Running threads with more frequent context switch...\n",
            "Threads, switch=1ms: 0.098 sec\n",
            "\n",
            "ASCII timeline (2 threads):\n",
            "\n",
            "t=0   [Thread-A RUN ▓▓▓]  GIL held\n",
            "      [Thread-B WAIT ..]\n",
            "\n",
            "t=0.005 context switch -> GIL to Thread-B\n",
            "      [Thread-A WAIT ..]\n",
            "      [Thread-B RUN ▓▓▓]\n",
            "\n",
            "CPU-bound threads simply alternate; they never run truly in parallel.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import threading\n",
        "import multiprocessing\n",
        "import sys\n",
        "import math\n",
        "\n",
        "# A CPU-bound task: compute many square roots\n",
        "def cpu_task(n):\n",
        "    total = 0.0\n",
        "    for i in range(1, n):\n",
        "        total += math.sqrt(i)\n",
        "    return total\n",
        "\n",
        "# Adjust N based on your machine speed to make the demo fast or slow\n",
        "N = 300_000  # ~300K iterations per run\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 1. Single-thread baseline\n",
        "# --------------------------\n",
        "print(\"Running single-threaded baseline...\")\n",
        "t0 = time.time()\n",
        "cpu_task(N)\n",
        "baseline = time.time() - t0\n",
        "print(f\"Single-thread : {round(baseline, 3)} sec\\n\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 2. Multi-threaded attempt\n",
        "# --------------------------\n",
        "# Threads won't run in parallel due to the GIL\n",
        "def threaded_run(workers=4):\n",
        "    threads = [threading.Thread(target=cpu_task, args=(N,)) for _ in range(workers)]\n",
        "    for th in threads:\n",
        "        th.start()\n",
        "    for th in threads:\n",
        "        th.join()\n",
        "\n",
        "print(\"Running multi-threaded (GIL limits parallelism)...\")\n",
        "t0 = time.time()\n",
        "threaded_run()\n",
        "threaded_time = time.time() - t0\n",
        "print(f\"4 threads     : {round(threaded_time, 3)} sec (GIL)\\n\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 3. Multi-processing version\n",
        "# --------------------------\n",
        "# Each process has its own GIL — true parallelism!\n",
        "def mp_worker(_):\n",
        "    cpu_task(N)\n",
        "\n",
        "def multiproc_run(workers=4):\n",
        "    with multiprocessing.Pool(workers) as pool:\n",
        "        pool.map(mp_worker, range(workers))\n",
        "\n",
        "print(\"Running multi-process (no shared GIL)...\")\n",
        "t0 = time.time()\n",
        "multiproc_run()\n",
        "mp_time = time.time() - t0\n",
        "print(f\"4 processes   : {round(mp_time, 3)} sec (no shared GIL)\\n\")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 4. sys.setswitchinterval demo\n",
        "# --------------------------\n",
        "# This controls how often Python switches between threads (~preemption interval)\n",
        "orig_si = sys.getswitchinterval()\n",
        "sys.setswitchinterval(0.001)  # Set to 1 millisecond (more frequent switching)\n",
        "\n",
        "print(\"Running threads with more frequent context switch...\")\n",
        "t0 = time.time()\n",
        "threaded_run()\n",
        "new_threaded_time = time.time() - t0\n",
        "print(f\"Threads, switch=1ms: {round(new_threaded_time, 3)} sec\")\n",
        "sys.setswitchinterval(orig_si)  # Restore default\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# ASCII Diagram\n",
        "# --------------------------\n",
        "print(r\"\"\"\n",
        "ASCII timeline (2 threads):\n",
        "\n",
        "t=0   [Thread-A RUN ▓▓▓]  GIL held\n",
        "      [Thread-B WAIT ..]\n",
        "\n",
        "t=0.005 context switch -> GIL to Thread-B\n",
        "      [Thread-A WAIT ..]\n",
        "      [Thread-B RUN ▓▓▓]\n",
        "\n",
        "CPU-bound threads simply alternate; they never run truly in parallel.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cProfile\n",
        "import pstats\n",
        "import io\n",
        "import tracemalloc\n",
        "import functools\n",
        "import sys\n",
        "\n",
        "\n",
        "# Increase recursion limit to allow deeper calls\n",
        "sys.setrecursionlimit(10000)\n",
        "\n",
        "\n",
        "# Naive Ackermann function (no memoization)\n",
        "def ack(m, n):\n",
        "    return n + 1 if m == 0 else (\n",
        "        ack(m - 1, 1) if n == 0 else ack(m - 1, ack(m, n - 1))\n",
        "    )\n",
        "\n",
        "\n",
        "# Memoized Ackermann using lru_cache\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def ack_mem(m, n):\n",
        "    return n + 1 if m == 0 else (\n",
        "        ack_mem(m - 1, 1) if n == 0 else ack_mem(m - 1, ack_mem(m, n - 1))\n",
        "    )\n",
        "\n",
        "\n",
        "# Profiling helper\n",
        "def profile(fn, *args):\n",
        "    pr = cProfile.Profile()\n",
        "    pr.enable()\n",
        "    result = fn(*args)\n",
        "    pr.disable()\n",
        "    s = io.StringIO()\n",
        "    ps = pstats.Stats(pr, stream=s).strip_dirs().sort_stats(pstats.SortKey.CUMULATIVE)\n",
        "    ps.print_stats(10)\n",
        "    return result, s.getvalue()\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Run both versions and collect stats\n",
        "# --------------------------\n",
        "\n",
        "print(\"Profiling naive ack(3,6)...\")\n",
        "tracemalloc.start()\n",
        "snap1 = tracemalloc.take_snapshot()\n",
        "result_naive, naive_stats = profile(ack, 3, 6)\n",
        "snap2 = tracemalloc.take_snapshot()\n",
        "top_mem_naive = snap2.compare_to(snap1, 'lineno')[:3]\n",
        "\n",
        "print(\"\\nProfiling memoized ack_mem(3,6)...\")\n",
        "snap3 = tracemalloc.take_snapshot()\n",
        "result_memo, memo_stats = profile(ack_mem, 3, 6)\n",
        "snap4 = tracemalloc.take_snapshot()\n",
        "top_mem_memo = snap4.compare_to(snap3, 'lineno')[:3]\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Print results\n",
        "# --------------------------\n",
        "\n",
        "print(\"\\n=== Naive cProfile top 10 ===\")\n",
        "print(naive_stats)\n",
        "\n",
        "print(\"=== Memoized cProfile top 10 ===\")\n",
        "print(memo_stats)\n",
        "\n",
        "print(\"=== Naive memory diff (top 3) ===\")\n",
        "for stat in top_mem_naive:\n",
        "    print(stat)\n",
        "\n",
        "print(\"\\n=== Memoized memory diff (top 3) ===\")\n",
        "for stat in top_mem_memo:\n",
        "    print(stat)\n",
        "\n",
        "print(\"\"\"\n",
        "Takeaways:\n",
        " - CPU time and memory allocations drop by orders of magnitude after memoization.\n",
        " - cProfile + tracemalloc combo highlights both compute and heap hotspots.\n",
        " - For recursive pure functions, caching is the first optimization to try.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgqirBVLYfx2",
        "outputId": "8547baae-e3a3-476e-bb4c-8ddffbdfa116"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiling naive ack(3,6)...\n",
            "\n",
            "Profiling memoized ack_mem(3,6)...\n",
            "\n",
            "=== Naive cProfile top 10 ===\n",
            "         172234 function calls (2 primitive calls) in 0.599 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            " 172233/1    0.599    0.000    0.599    0.599 <ipython-input-5-62bc0a52f059>:14(ack)\n",
            "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
            "\n",
            "\n",
            "\n",
            "=== Memoized cProfile top 10 ===\n",
            "         1278 function calls (2 primitive calls) in 0.004 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "   1277/1    0.004    0.000    0.004    0.004 <ipython-input-5-62bc0a52f059>:21(ack_mem)\n",
            "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
            "\n",
            "\n",
            "\n",
            "=== Naive memory diff (top 3) ===\n",
            "<ipython-input-4-e74cd837cf8c>:16: size=40.9 KiB (-40.0 KiB), count=747 (-127), average=56 B\n",
            "/usr/lib/python3.11/tracemalloc.py:558: size=39.0 KiB (+33.8 KiB), count=748 (+642), average=53 B\n",
            "/usr/lib/python3.11/tokenize.py:532: size=17.5 KiB (-22.4 KiB), count=320 (-410), average=56 B\n",
            "\n",
            "=== Memoized memory diff (top 3) ===\n",
            "<ipython-input-5-62bc0a52f059>:24: size=81.4 KiB (+81.4 KiB), count=884 (+884), average=94 B\n",
            "/usr/lib/python3.11/tracemalloc.py:558: size=91.8 KiB (+32.9 KiB), count=1760 (+647), average=53 B\n",
            "<ipython-input-4-e74cd837cf8c>:16: size=0 B (-20.9 KiB), count=0 (-382)\n",
            "\n",
            "Takeaways:\n",
            " - CPU time and memory allocations drop by orders of magnitude after memoization.\n",
            " - cProfile + tracemalloc combo highlights both compute and heap hotspots.\n",
            " - For recursive pure functions, caching is the first optimization to try.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}